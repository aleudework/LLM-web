import json
import trafilatura
import requests
from duckduckgo_search import DDGS
from ollama import Client

ollama = Client()

# üîç Funktion som s√∏ger og henter ren tekst
def search_web(query: str) -> str:
    print(f"[!] S√∏ger p√• nettet: {query}")
    with DDGS() as ddgs:
        results = ddgs.text(query, max_results=3)
        urls = [r['href'] for r in results]

    content_list = []
    for url in urls:
        try:
            html = requests.get(url, timeout=10).text
            clean_text = trafilatura.extract(html)
            if clean_text:
                content_list.append(clean_text)
        except Exception as e:
            print(f"Kunne ikke hente {url}: {e}")

    return "\n\n".join(content_list[:2])  # Begr√¶ns l√¶ngde

# üß∞ Fort√¶l Ollama hvilke funktioner den m√• bruge
functions = [
    {
        "name": "search_web",
        "description": "S√∏ger p√• nettet og returnerer nyttig tekst",
        "parameters": {
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "Det sp√∏rgsm√•l eller emne der skal s√∏ges efter"
                }
            },
            "required": ["query"]
        }
    }
]

# ü§ñ Start en samtale med LLM
def chat_with_llm(question: str):
    messages = [{"role": "user", "content": question}]

    response = ollama.chat(
        model="llama3",
        messages=messages,
        functions=functions,
        function_call="auto"
    )

    # üìå Hvis modellen vil bruge en funktion
    if "function_call" in response:
        name = response["function_call"]["name"]
        args = json.loads(response["function_call"]["arguments"])

        if name == "search_web":
            result = search_web(args["query"])
            messages.append(response)  # tilf√∏j modelens call
            messages.append({
                "role": "function",
                "name": name,
                "content": result
            })

            # üîÅ Giv svaret fra funktionen til LLM og f√• endeligt svar
            final_response = ollama.chat(
                model="llama3",
                messages=messages
            )
            return final_response["message"]["content"]
    else:
        return response["message"]["content"]

# ‚úÖ Eksempel
if __name__ == "__main__":
    query = input("Hvad vil du sp√∏rge om? ")
    svar = chat_with_llm(query)
    print("\nü§ñ Svar fra LLM:\n", svar)